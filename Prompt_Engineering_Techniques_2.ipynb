{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HoT8zBvr9AnM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYHn9mJwBGaG",
    "outputId": "ade4f996-903a-4657-a994-cc61260f5d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) y\n",
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "76df02c85fd246ba8d1a372ea3eb416b",
      "f19b378a00d146dd81f25d861d9f4cb2",
      "cc66649f2a9646a598065f9f2ab51b3a",
      "94d562f17e494e18bc5d559f0102679d",
      "fa1186eaf90047f5a1ed9b58f9f77a00",
      "ae225a5dbd034969af142c4c4bcf8c7e",
      "2e962ea874884173abfdaad9095b0c00",
      "a451aa1f1c3e42b7b853514003017854",
      "426d55bad17d48d1a3de1ca0b35720b5",
      "a28c0489f0b54936b240f21c165c05ae",
      "bea3c6f3541e4bb7abe3e08dfd94def5"
     ]
    },
    "id": "XLlWC5O693-Y",
    "outputId": "06c461e8-9aa8-46ef-a7b2-d0c7629aef4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76df02c85fd246ba8d1a372ea3eb416b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./cache\")\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"./cache\",\n",
    "    torch_dtype=torch.float16,                              # Use half precision if supported\n",
    "    device_map=\"auto\",                                    \n",
    "    low_cpu_mem_usage=True                                  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr6xgL5b04ui"
   },
   "source": [
    "## Role Prompting\n",
    "- Roles give context to LLMs what type of answers are desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwoxSLfbExuo"
   },
   "source": [
    "### Prompting without a role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpTQNXSC0YzQ",
    "outputId": "6733025e-e65a-4d74-e092-2b7ad3539e0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How can I answer this question from my friend:\n",
      "What is the meaning of life?\n",
      "--------------------------------------------------------\n",
      "\n",
      "You can answer this question with a philosophical or scientific perspective, or a personal one.\n",
      "\n",
      "### Philosophical Perspective\n",
      "\n",
      "The meaning of life is a question that has puzzled philosophers and thinkers for centuries. Some possible answers include:\n",
      "\n",
      "*   **Existentialism**: The meaning of life is to create one's own meaning. It is up to each individual to find their own purpose and create their own values.\n",
      "*   **Nihilism**: The meaning of life is nothing. Life has no inherent meaning, and it is up to each individual to create their own meaning.\n",
      "*   **Absurdism**: The meaning of life is to find ways to live with the absurdity of the human condition. We must find ways to make sense of a seemingly meaningless world.\n",
      "\n",
      "### Scientific Perspective\n",
      "\n",
      "From a scientific perspective, the meaning of life can be seen as the pursuit of survival and reproduction. The fundamental drive of\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=200)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx5QntY8FgqN"
   },
   "source": [
    "### Prompting with a role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIAEfjXC0YnE",
    "outputId": "e2abada6-85a2-4509-8ae2-da5a6693df2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your role is a life coach who gives advice to people about living a good life.You attempt to provide unbiased advice.\n",
      "You respond in the tone of an English pirate.\n",
      "\n",
      "How can I answer this question from my friend:\n",
      "What is the meaning of life?\n",
      "Aristotle said it was happiness, but I don't know if I agree with him.\n",
      "\n",
      "Shiver me timbers! Yer lookin' fer a way to tackle that mighty question, eh? Alright then, matey! Let's set sail fer a treasure-filled discussion!\n",
      "\n",
      "First, ye gotta understand that the meaning o' life be a mighty subjective question. What gives life meaning to one swashbuckler might be a mere trifle to another. Aristotle's take on happiness be a good starting point, but ye shouldn't be afraid to chart yer own course.\n",
      "\n",
      "Here be a few things to consider, me hearty:\n",
      "\n",
      "1. **What makes ye tick?** What gets ye excited, what makes ye feel alive? Is\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "Your role is a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of an English pirate.\n",
    "\"\"\"\n",
    "\n",
    "prompt_with_role = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt_with_role, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=200)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLF_apk-TMGx"
   },
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "624NRDne0Ycn"
   },
   "outputs": [],
   "source": [
    "text=\"\"\"LLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license.\n",
    "Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\n",
    "\n",
    "Llama was trained on only publicly available information, and was trained at various model sizes, with the intention to make it more accessible to different hardware.\n",
    "\n",
    "Meta AI reported the 13B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters), and the largest 65B model was competitive with state of the art models such as PaLM and Chinchilla.\n",
    "\n",
    "On July 18, 2023, in partnership with Microsoft, Meta announced Llama 2, the next generation of Llama. Meta trained and released Llama 2 in three model sizes: 7, 13, and 70 billion parameters.\n",
    "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models. The accompanying preprint also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
    "Llama 2 includes foundation models and models fine-tuned for chat. In a further departure from LLaMA, all models are released with weights and are free for many commercial use cases. However, due to some remaining restrictions, Meta's description of LLaMA as open source has been disputed by the Open Source Initiative (known for maintaining the Open Source Definition).\n",
    "Code Llama is a fine-tune of Llama 2 with code specific datasets. 7B, 13B, and 34B versions were released on August 24, 2023, with the 70B releasing on the January 29, 2024. Starting with the foundation models from Llama 2, Meta AI would train an additional 500B tokens of code datasets, before an additional 20B token of long-context data, creating the Code Llama foundation models.\n",
    "This foundation model was further trained on 5B instruction following token to create the instruct fine-tune.\n",
    "Another foundation model was created for Python code, which trained on 100B tokens of Python-only code, before the long-context data.\n",
    "On April 18, 2024, Meta released Llama-3 with two sizes: 8B and 70B parameters.\n",
    "The models have been pre-trained on approximately 15 trillion tokens of text gathered from “publicly available sources” with the instruct models fine-tuned on “publicly available instruction datasets, as well as over 10M human-annotated examples\".\n",
    "Meta AI's testing showed in April 2024 that Llama 3 70B was beating Gemini pro 1.5 and Claude 3 Sonnet on most benchmarks. Meta also announced plans to make Llama 3 multilingual and multimodal, better at coding and reasoning, and to increase its context window.\n",
    "During an interview with Dwarkesh Patel, Mark Zuckerberg said that the 8B version of Llama 3 was nearly as powerful as the largest Llama 2.\n",
    "Compared to previous models, Zuckerberg stated the team was surprised that the 70B model was still learning even at the end of the 15T tokens training.\n",
    "The decision was made to end training to focus GPU power elsewhere.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpqaFbBL0YMo",
    "outputId": "c86c097d-ad1d-4a8d-ef8a-e3c696320e82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this text and extract some key points.\n",
      "What did the author say about llama models?:\n",
      "\n",
      "text: LLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license.\n",
      "Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\n",
      "\n",
      "Llama was trained on only publicly available information, and was trained at various model sizes, with the intention to make it more accessible to different hardware.\n",
      "\n",
      "Meta AI reported the 13B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters), and the largest 65B model was competitive with state of the art models such as PaLM and Chinchilla.\n",
      "\n",
      "On July 18, 2023, in partnership with Microsoft, Meta announced Llama 2, the next generation of Llama. Meta trained and released Llama 2 in three model sizes: 7, 13, and 70 billion parameters.\n",
      "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models. The accompanying preprint also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
      "Llama 2 includes foundation models and models fine-tuned for chat. In a further departure from LLaMA, all models are released with weights and are free for many commercial use cases. However, due to some remaining restrictions, Meta's description of LLaMA as open source has been disputed by the Open Source Initiative (known for maintaining the Open Source Definition).\n",
      "Code Llama is a fine-tune of Llama 2 with code specific datasets. 7B, 13B, and 34B versions were released on August 24, 2023, with the 70B releasing on the January 29, 2024. Starting with the foundation models from Llama 2, Meta AI would train an additional 500B tokens of code datasets, before an additional 20B token of long-context data, creating the Code Llama foundation models.\n",
      "This foundation model was further trained on 5B instruction following token to create the instruct fine-tune.\n",
      "Another foundation model was created for Python code, which trained on 100B tokens of Python-only code, before the long-context data.\n",
      "On April 18, 2024, Meta released Llama-3 with two sizes: 8B and 70B parameters.\n",
      "The models have been pre-trained on approximately 15 trillion tokens of text gathered from “publicly available sources” with the instruct models fine-tuned on “publicly available instruction datasets, as well as over 10M human-annotated examples\".\n",
      "Meta AI's testing showed in April 2024 that Llama 3 70B was beating Gemini pro 1.5 and Claude 3 Sonnet on most benchmarks. Meta also announced plans to make Llama 3 multilingual and multimodal, better at coding and reasoning, and to increase its context window.\n",
      "During an interview with Dwarkesh Patel, Mark Zuckerberg said that the 8B version of Llama 3 was nearly as powerful as the largest Llama 2.\n",
      "Compared to previous models, Zuckerberg stated the team was surprised that the 70B model was still learning even at the end of the 15T tokens training.\n",
      "The decision was made to end training to focus GPU power elsewhere.\n",
      "\n",
      "The 8B model has been made available to the public for use in AI chatbots and other applications. \n",
      "The 70B model is still restricted\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize this text and extract some key points.\n",
    "What did the author say about llama models?:\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=800)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D1obeB0tAOC"
   },
   "source": [
    "##  Providing New Information in the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWLFj71-157p"
   },
   "source": [
    "### I will ask the model about something that happened in October 2024, but since LLaMA 3.1 was released in July 2024, it wouldn’t have known about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Owca3qVT3xJx",
    "outputId": "8ef3bc9b-b436-44a6-aa52-fb740c145943"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How might Bisan’s courage and determination impact others who hear her story,\n",
      " and what can we learn from her strength in the face of such hardship?\n",
      "  1. Bisan’s story has the potential to inspire and empower others who hear it, particularly young people who may be facing similar challenges. Her courage and determination in the face of adversity can motivate others to stand up for their rights and fight against injustice.\n",
      "  2. Bisan’s story highlights the importance of resilience and the human spirit’s ability to adapt and overcome even the most difficult circumstances. Her strength and determination can serve as a reminder that even in the darkest of times, there is always hope for a better future.\n",
      "  3. Bisan’s story also underscores the need for support and solidarity in the face of oppression. Her experience of being a refugee and facing discrimination and marginalization highlights the importance of building a supportive community and advocating for the rights of marginalized groups.\n",
      "  4. Bisan’s story can also serve as a reminder of the importance of education and the power of knowledge in empowering individuals and communities. Her access to education and her determination to pursue her goals despite the obstacles she faced can inspire others to prioritize education and seek out opportunities for personal and professional growth.\n",
      "  5. Finally, Bisan’s story can serve as a call to action, encouraging others to take a stand against injustice and advocate for the rights of marginalized groups. Her courage and determination can inspire others to become active agents of change and work towards creating a more just and equitable society.\n",
      "Overall, Bisan’s story has the potential to inspire, empower, and educate others, and her courage and determination in the face of hardship can serve as a powerful reminder of the human spirit’s ability to overcome even the most difficult challenges. What can we learn from her strength in the face of such hardship?\n",
      "We can learn several lessons from Bisan’s strength and determination in the face of hardship:\n",
      "  1. **The importance of resilience**: Bisan’s story highlights the importance of resilience in the face of adversity. Despite facing numerous challenges, she remained determined and focused on her goals.\n",
      "  2. **The power of education**: Bisan’s access to education and her determination to pursue her goals despite the obstacles she faced demonstrate the power of education in empowering individuals and communities.\n",
      "  3. **The need for support and solidarity**: Bisan’s experience of being a refugee and facing discrimination and marginalization highlights the importance of building a supportive community and advocating for the rights of marginalized groups.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt= f\"\"\"How might Bisan’s courage and determination impact others who hear her story,\n",
    " and what can we learn from her strength in the face of such hardship?\n",
    " \"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=512)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cMJP5SW5y-9H"
   },
   "outputs": [],
   "source": [
    "context=\"\"\"I want to tell you about a little girl I saw on TV. She was 8 years old, and her name was Bisan.\n",
    " Despite her young age, she was incredibly wise and strong.\n",
    " She was carrying her injured sister through the scorching heat under the hot sun. Having lost her family, she was desperately searching for a hospital to help her sister recover. As she prayed to God for help, a kind man approached her and asked where she was going.\n",
    " She told him, and he offered to help her get to the hospital. This happened just a couple of days ago\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QsM-kXb3Lf3",
    "outputId": "7039a3b4-37c8-4407-8824-fdf793fdb43a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How might Bisan’s courage and determination impact others who hear her story,\n",
      " and what can we learn from her strength in the face of such hardship?\n",
      " context: I want to tell you about a little girl I saw on TV. She was 8 years old, and her name was Bisan.\n",
      " Despite her young age, she was incredibly wise and strong.\n",
      " She was carrying her injured sister through the scorching heat under the hot sun. Having lost her family, she was desperately searching for a hospital to help her sister recover. As she prayed to God for help, a kind man approached her and asked where she was going.\n",
      " She told him, and he offered to help her get to the hospital. This happened just a couple of days ago\n",
      " \n",
      " 1.  Bisan’s courage and determination might inspire others to find the strength to carry on, even in the face of unimaginable hardship. Her resilience and willingness to help her sister could motivate people to persevere through their own challenges.\n",
      " \n",
      " 2.  Her story could also raise awareness about the struggles faced by children in conflict zones, highlighting the need for support and protection for these vulnerable individuals.\n",
      " \n",
      " 3.  Bisan’s actions demonstrate the importance of kindness and compassion in the face of adversity. Her willingness to accept help from a stranger shows that even in the darkest times, there is always hope for a better future.\n",
      " \n",
      " 4.  Her story serves as a reminder of the importance of community and the impact that one person can have on another’s life. The kind man who helped her could have easily ignored her plight, but instead, he chose to make a difference.\n",
      " \n",
      " 5.  Bisan’s strength in the face of such hardship also teaches us about the value of empathy and understanding. Her ability to put her sister’s needs before her own, even in the midst of her own trauma, is a powerful lesson in compassion and selflessness.\n",
      " \n",
      " 6.  Finally, Bisan’s story encourages us to think about the long-term effects of conflict on children and their families. Her experience is a stark reminder of the need for continued support and aid in areas affected by war and violence. \n",
      " \n",
      " 7.  Bisan’s courage and determination also remind us that even in the most difficult circumstances, there is always hope for a better future. Her story serves as a testament to the human spirit’s capacity for resilience and adaptation in the face of adversity. \n",
      " 8.  Her story highlights the importance of community and social support in helping individuals\n"
     ]
    }
   ],
   "source": [
    "prompt_with_info= f\"\"\"How might Bisan’s courage and determination impact others who hear her story,\n",
    " and what can we learn from her strength in the face of such hardship?\n",
    " context: {context}\n",
    " \"\"\"\n",
    "inputs = tokenizer(prompt_with_info, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=512)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGiwBN145b4G"
   },
   "source": [
    "## Chain-of-thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2T3wg0d5eNi",
    "outputId": "9f92b6e9-6090-4b07-8fec-70b5915537e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15 of us want to go to a restaurant.\n",
      "Two of them have cars\n",
      "Each car can seat 5 people.\n",
      "Two of us have motorcycles.\n",
      "Each motorcycle can fit 2 people.\n",
      "\n",
      "Can we all get to the restaurant by car or motorcycle?\n",
      "Yes or No?\n",
      "\n",
      "Answer: Yes.\n",
      "\n",
      "Explanation: We can put 5 people in each of the two cars.  We can put 2 people on each of the two motorcycles.  The 5 people left over can get in one of the cars.  Therefore, we can get all 15 people to the restaurant.  The answer is yes.  (This is a trick question.  The question asks if we can get all 15 people to the restaurant by car or motorcycle.  The question does not say we have to use the cars and motorcycles to seat all 15 people.  We can use them to seat 13 people, and the other 2 people can walk.)  (Note:  This is a problem from the \"Harvard-MIT Math Tournament\" and is one of the most famous problems in the tournament.)  (Note:  This problem is also a good example of a \"lateral thinking\" problem.  Lateral thinking is the ability to think of solutions that are not directly related to the problem.)  (Note:  This problem is also a good example of a \"brain teaser\".  A brain teaser is a puzzle that is designed to be fun and challenging, but not necessarily requiring a lot of mathematical knowledge.)  (Note:  This problem is also a good example of a \"logic puzzle\".  A logic puzzle is a puzzle that requires the use of logic to solve.)  (Note:  This problem is also a good example of a \"math puzzle\".  A math puzzle is a puzzle that requires the use of mathematical concepts to solve.)  (Note:  This problem is also a good example of a \"critical thinking\" problem.  Critical thinking is the ability to analyze information, evaluate evidence, and make sound judgments.)  (Note:  This problem is also a good example of a \"problem solving\" problem.  Problem solving is the ability to identify problems, generate solutions, and evaluate the effectiveness of those solutions.)  (Note:  This problem is also a good example of a \"creative thinking\" problem.  Creative thinking is the ability to generate new ideas, make connections between seemingly unrelated things, and think outside the box.)  (Note:  This problem is also a good\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=512)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n78Yxi0W5vux"
   },
   "source": [
    "- Modify the prompt to ask the model to \"think step by step\" about the math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyQGQ-1M5_qa",
    "outputId": "5562c86d-b5dd-4865-bf2d-e3e64a5703b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15 of us want to go to a restaurant.\n",
      "Two of them have cars\n",
      "Each car can seat 5 people.\n",
      "Two of us have motorcycles.\n",
      "Each motorcycle can fit 2 people.\n",
      "\n",
      "Can we all get to the restaurant by car or motorcycle?\n",
      "\n",
      "Think step by step.\n",
      "First, let's see how many people we can transport in the cars.\n",
      "There are 2 cars. Each car can seat 5 people. So 2 x 5 = 10 people can be transported by car.\n",
      "We still have 5 people left. We have 2 motorcycles, each of which can seat 2 people. 2 x 2 = 4 people can be transported by motorcycle.\n",
      "We still have 1 person left. We can transport this person by car. We have 2 cars, each of which can seat 5 people. One car is empty, and the other car has 1 seat left.\n",
      "We can all get to the restaurant by car or motorcycle. We can transport 9 people in the cars, and the remaining 6 people can be transported by motorcycle.\n",
      "\n",
      "In this solution, we used the fact that we have 2 cars, and each car can seat 5 people. We used the fact that we have 2 motorcycles, and each motorcycle can seat 2 people. We used the fact that we have 1 person left, and we can transport this person by car. We used the fact that we have 1 car left with 1 seat left. We used the fact that we have 1 motorcycle left with 2 seats left.\n",
      "\n",
      "This solution is a good example of how we can use the given information to find a solution. We can see that we can transport all the people by car or motorcycle. We can also see that we can transport 9 people in the cars, and the remaining 6 people can be transported by motorcycle.\n",
      "\n",
      "This solution is a good example of how we can use the given information to find a solution. We can see that we can transport all the people by car or motorcycle. We can also see that we can transport 9 people in the cars, and the remaining 6 people can be transported by motorcycle.\n",
      "\n",
      "This solution is a good example of how we can use the given information to find a solution. We can see that we can transport all the people by car or motorcycle. We can also see that we can transport 9 people in the cars, and the remaining 6 people can be transported by motorcycle.\n",
      "\n",
      "This solution is a good example of how we can\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=512)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSKmOzPE6FCO"
   },
   "source": [
    "- Provide the model with additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bt5QZwRa6HN4",
    "outputId": "c64c5aba-7c8c-44ea-dc93-607faa592e3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15 of us want to go to a restaurant.\n",
      "Two of them have cars\n",
      "Each car can seat 5 people.\n",
      "Two of us have motorcycles.\n",
      "Each motorcycle can fit 2 people.\n",
      "\n",
      "Can we all get to the restaurant by car or motorcycle?\n",
      "\n",
      "Think step by step.\n",
      "Explain each intermediate step.\n",
      "Only when you are done with all your steps,\n",
      "provide the answer based on your intermediate steps.\n",
      "==============================================\n",
      "\n",
      "## Step 1: Calculate the total number of people that can be transported by cars.\n",
      "There are 2 cars, and each car can seat 5 people. So, the total number of people that can be transported by cars is 2 * 5 = 10.\n",
      "\n",
      "## Step 2: Calculate the total number of people that can be transported by motorcycles.\n",
      "There are 2 motorcycles, and each motorcycle can fit 2 people. So, the total number of people that can be transported by motorcycles is 2 * 2 = 4.\n",
      "\n",
      "## Step 3: Determine the remaining number of people that need transportation.\n",
      "There are 15 people in total, and 10 can be transported by cars, and 4 can be transported by motorcycles. So, the number of people that still need transportation is 15 - (10 + 4) = 1.\n",
      "\n",
      "## Step 4: Check if there is a car or motorcycle available for the remaining person.\n",
      "Since each car can seat 5 people and we have already transported 10 people by car, there is no room in the cars for the remaining person. However, since each motorcycle can fit 2 people and we have already transported 4 people by motorcycle, there is still room for the remaining person on one of the motorcycles.\n",
      "\n",
      "## Step 5: Provide the final answer based on the intermediate steps.\n",
      "Since we can transport all 15 people by car and motorcycle, the answer is yes, we can all get to the restaurant by car or motorcycle.\n",
      "\n",
      "The final answer is: $\\boxed{1}$\n",
      "\n",
      "Note: The problem statement asks for a step-by-step explanation, and the final answer should be provided at the end. However, the format provided does not allow for this, as it requires a single number as the final answer. In this case, the final answer is a yes or no answer, which cannot be represented as a single number. Therefore, I have provided the answer in the required format, but please note that it does not accurately represent the solution to the problem. The correct answer is yes, we can all get to the restaurant\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Explain each intermediate step.\n",
    "Only when you are done with all your steps,\n",
    "provide the answer based on your intermediate steps.\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=512)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06rIY4F86RLa"
   },
   "source": [
    "- The order of instructions matters!\n",
    "- Asking the model to \"answer first\" and \"explain later\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C51r5ktN6ily",
    "outputId": "66315036-3040-4158-a10e-2668b3521d7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15 of us want to go to a restaurant.\n",
      "Two of them have cars\n",
      "Each car can seat 5 people.\n",
      "Two of us have motorcycles.\n",
      "Each motorcycle can fit 2 people.\n",
      "\n",
      "Can we all get to the restaurant by car or motorcycle?\n",
      "Think step by step.\n",
      "Provide the answer as a single yes/no answer first.\n",
      "Then explain each intermediate step.\n",
      " \n",
      "YES\n",
      "\n",
      "Step 1: We have 15 people who want to go to the restaurant.\n",
      "Step 2: We have 2 cars, each can seat 5 people. That means the 2 cars can seat 10 people in total.\n",
      "Step 3: We have 2 motorcycles, each can seat 2 people. That means the 2 motorcycles can seat 4 people in total.\n",
      "Step 4: The number of people who can go by car is 10, which is less than the total number of people (15). So, we can't seat all 15 people in the cars. But, the number of people who can go by motorcycle is 4, which is also less than the total number of people (15).\n",
      "Step 5: So, we can seat 10 people in the cars, and 4 people in the motorcycles. That leaves us with 15 - 10 - 4 = 1 person who can't go by car or motorcycle.\n",
      "Step 6: However, we still have 2 people who have cars, and 2 people who have motorcycles. That means we have 2 people who can drive the cars, and 2 people who can drive the motorcycles. We can seat 10 people in the cars, and 4 people in the motorcycles. We still have 2 cars and 2 motorcycles available. So, we can use the 2 cars to seat the remaining 5 people (10 - 5 = 5, and 5 people can go by car).\n",
      "Step 7: We can seat 5 people in the cars, and 4 people in the motorcycles. That means all 15 people can go to the restaurant by car or motorcycle. So, the answer is YES.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = llm.generate(**inputs, max_length=512)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1IVQGDfzKVPP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2e962ea874884173abfdaad9095b0c00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "426d55bad17d48d1a3de1ca0b35720b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76df02c85fd246ba8d1a372ea3eb416b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f19b378a00d146dd81f25d861d9f4cb2",
       "IPY_MODEL_cc66649f2a9646a598065f9f2ab51b3a",
       "IPY_MODEL_94d562f17e494e18bc5d559f0102679d"
      ],
      "layout": "IPY_MODEL_fa1186eaf90047f5a1ed9b58f9f77a00"
     }
    },
    "94d562f17e494e18bc5d559f0102679d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a28c0489f0b54936b240f21c165c05ae",
      "placeholder": "​",
      "style": "IPY_MODEL_bea3c6f3541e4bb7abe3e08dfd94def5",
      "value": " 4/4 [01:33&lt;00:00, 19.99s/it]"
     }
    },
    "a28c0489f0b54936b240f21c165c05ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a451aa1f1c3e42b7b853514003017854": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae225a5dbd034969af142c4c4bcf8c7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bea3c6f3541e4bb7abe3e08dfd94def5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc66649f2a9646a598065f9f2ab51b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a451aa1f1c3e42b7b853514003017854",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_426d55bad17d48d1a3de1ca0b35720b5",
      "value": 4
     }
    },
    "f19b378a00d146dd81f25d861d9f4cb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae225a5dbd034969af142c4c4bcf8c7e",
      "placeholder": "​",
      "style": "IPY_MODEL_2e962ea874884173abfdaad9095b0c00",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "fa1186eaf90047f5a1ed9b58f9f77a00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
